{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhis\\AppData\\Local\\Temp\\ipykernel_4520\\237286440.py:4: DtypeWarning: Columns (2,4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_ms_bucket= pd.read_csv(file_path)\n",
      "c:\\Users\\abhis\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\dtypes\\cast.py:1057: RuntimeWarning: invalid value encountered in cast\n",
      "  if (arr.astype(int) == arr).all():\n",
      "c:\\Users\\abhis\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\dtypes\\cast.py:1081: RuntimeWarning: invalid value encountered in cast\n",
      "  if (arr.astype(int) == arr).all():\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "file_path = 'C:/Users/abhis/Desktop/CloudBuilders/Grafana-Anomaly/Grafana-Anomaly_Detection/Anomaly_grafana/notebooks/output_csv/duration_milliseconds_bucket.csv'  # Replace this with your file path \n",
    "df_ms_bucket= pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "df_ms_bucket.head()\n",
    "\n",
    "#NAN VALUE TREATMENT------------------------------------------------------------------\n",
    "# Display the number of NaN values in each column\n",
    "df_ms_bucket.isna().sum()\n",
    "\n",
    "#Interpolate\n",
    "df_converted = df_ms_bucket.convert_dtypes()\n",
    "\n",
    "# Select only numeric columns for interpolation\n",
    "numeric_cols = df_converted.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Perform interpolation on numeric columns only\n",
    "df_converted[numeric_cols] = df_converted[numeric_cols].interpolate()\n",
    "\n",
    "# Check for any remaining NaN values\n",
    "df_converted.isna().sum()\n",
    "\n",
    "#Backward Fill\n",
    "df_bfill = df_converted.bfill()\n",
    "df_bfill.isna().sum()\n",
    "\n",
    "#Forward Fill\n",
    "df_ffill = df_bfill.ffill()\n",
    "df_ffill.isna().sum()\n",
    "\n",
    "#creating a final df\n",
    "final_df = df_ffill\n",
    "final_df.head()\n",
    "\n",
    "#optional\n",
    "#final_df.to_csv('C:/Users/abhis/Desktop/CloudBuilders/Grafana-Anomaly/Grafana-Anomaly_Detection/Anomaly_grafana/dataframes/duration_ms_bucket_df.csv', index=False)\n",
    "\n",
    "\n",
    "# Check data types\n",
    "#print(final_df.dtypes)\n",
    "\n",
    "#summary statistics\n",
    "#final_df.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model with numerical and categorical anomalies (isolation_forest)\n",
    "\n",
    "#DATA PROCESSING-----------------------------------------------------------------------\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Selecting the numerical and categorical columns\n",
    "numerical_columns = ['Value', 'http_status_code']\n",
    "categorical_columns = ['exported_instance', 'exported_job', 'http_method', \n",
    "                       'http_route', 'instance', 'job', 'service_name', \n",
    "                       'span_kind', 'span_name', 'status_code']\n",
    "\n",
    "# Preprocessing: Scaling numerical data and encoding categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_columns),\n",
    "        ('cat', OneHotEncoder(), categorical_columns)])\n",
    "\n",
    "# Creating the Isolation Forest model pipeline\n",
    "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                        ('model', IsolationForest(contamination=0.001, random_state=42))])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(final_df)\n",
    "\n",
    "# Predict anomalies (anomalies will be labeled as -1, normal as 1)\n",
    "final_df['anomaly'] = model.fit_predict(final_df)\n",
    "\n",
    "# Filter the anomalies\n",
    "anomalies = final_df[final_df['anomaly'] == -1]\n",
    "\n",
    "# Display the detected anomalies\n",
    "anomalies\n",
    "\n",
    "# PLOT THE ANOMALY\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert 'Time' to datetime format\n",
    "final_df['Time'] = pd.to_datetime(final_df['Time'])\n",
    "\n",
    "# Plotting the time series data with anomalies\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot all data points\n",
    "plt.plot(final_df['Time'], final_df['Value'], label='Normal', color='blue', alpha=0.5)\n",
    "\n",
    "# Highlight the anomalies\n",
    "anomalies = final_df[final_df['anomaly'] == -1]\n",
    "plt.scatter(anomalies['Time'], anomalies['Value'], color='red', label='Anomalies')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.title('duration_milliseconds_bucket Isolation_Forest(num+cat)')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "print(f\"Number of anomalies detected: {len(anomalies)}\")\n",
    "print(anomalies.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model with only numerical anomalies (isolation_forest)\n",
    "\n",
    "# Convert 'Time' column to datetime and set as index\n",
    "final_df['Time'] = pd.to_datetime(final_df['Time'])\n",
    "final_df.set_index('Time', inplace=True)\n",
    " \n",
    "# Select all relevant numeric metric columns for anomaly detection\n",
    "metrics_columns = ['Value']  # Add other metric columns as needed\n",
    "metrics_data = final_df[metrics_columns]\n",
    " \n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "metrics_scaled = scaler.fit_transform(metrics_data)\n",
    " \n",
    "# Initialize the Isolation Forest model\n",
    "iso_forest = IsolationForest(n_estimators=100, contamination=0.001, random_state=42)\n",
    " \n",
    "# Fit the model and predict anomalies (-1 means anomaly, 1 means normal)\n",
    "final_df['anomaly'] = iso_forest.fit_predict(metrics_scaled)\n",
    " \n",
    "# Visualize the anomalies for each metric\n",
    "for column in metrics_columns:\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(final_df.index, final_df[column], label=column, color='blue')\n",
    "    plt.scatter(final_df.index[final_df['anomaly'] == -1], final_df[column][final_df['anomaly'] == -1],\n",
    "                color='red', label='Anomaly', marker='x')\n",
    "    plt.title(f'duration_milliseconds_bucket Isolation_Forest(num)')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(column)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    " \n",
    "# Print out the anomalies\n",
    "anomalies = final_df[final_df['anomaly'] == -1]\n",
    "print(f\"Number of anomalies detected: {len(anomalies)}\")\n",
    "print(anomalies.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z-Score \n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Load your DataFrame (df) here\n",
    "# df = pd.read_csv('your_file.csv')\n",
    "\n",
    "# Compute z-scores for the \"Value\" column\n",
    "final_df['Value_zscore'] = zscore(final_df['Value'])\n",
    "\n",
    "# Set a threshold for anomaly detection (e.g., 3 standard deviations)\n",
    "threshold =0.1\n",
    "\n",
    "# Flag anomalies (True for anomalies, False otherwise)\n",
    "final_df['Anomaly'] = (final_df['Value_zscore'].abs() > threshold)\n",
    "\n",
    "# Display the rows with anomalies\n",
    "anomalies = final_df[final_df['Anomaly']]\n",
    "\n",
    "# Print anomalies or handle them as needed\n",
    "#print(anomalies)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the anomaly detection threshold\n",
    "#threshold = 1\n",
    "\n",
    "# Flag anomalies (True for anomalies, False otherwise)\n",
    "final_df['Anomaly'] = (final_df['Value_zscore'].abs() > threshold)\n",
    "\n",
    "# Plotting the data points and highlighting anomalies\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(final_df.index, final_df['Value'], label='Data Points')\n",
    "plt.scatter(final_df[final_df['Anomaly']].index, final_df[final_df['Anomaly']]['Value'], color='red', label='Anomalies', marker='x')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Value')\n",
    "plt.title('duration_milliseconds_bucket Z-Score Model')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f\"Number of anomalies detected: {len(anomalies)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IQR\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Calculate Q1 (25th percentile) and Q3 (75th percentile) for the 'Value' column\n",
    "Q1 = final_df['Value'].quantile(0.25)\n",
    "Q3 = final_df['Value'].quantile(0.75)\n",
    "\n",
    "# Step 2: Calculate IQR (Interquartile Range)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Step 3: Define the lower and upper bounds for anomalies\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Step 4: Identify anomalies\n",
    "anomalies = final_df[(final_df['Value'] < lower_bound) | (final_df['Value'] > upper_bound)]\n",
    "\n",
    "print(f\"Number of anomalies detected: {len(anomalies)}\")\n",
    "\n",
    "# Step 5: Plotting the graph without converting 'Time' to datetime\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot all values\n",
    "plt.plot(final_df.index, final_df['Value'], label='Value', color='blue')\n",
    "\n",
    "# Highlight anomalies\n",
    "plt.scatter(final_df[final_df['Anomaly']].index, final_df[final_df['Anomaly']]['Value'], color='red', label='Anomalies')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('duration_milliseconds_bucket IQR Model')\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.xticks(rotation=45)  # Rotate x labels for better readability\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping of labels\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-Class SVM Model\n",
    "\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the data\n",
    "#file_path = 'C:/Users/abhis/Desktop/CloudBuilders/Grafana-Anomaly/Grafana-Anomaly_Detection/Anomaly_grafana/dataframes/calls_total_df.csv'\n",
    "#final_df = pd.read_csv(file_path)\n",
    "\n",
    "# Strip any leading/trailing spaces from column names\n",
    "final_df.columns = final_df.columns.str.strip()\n",
    "\n",
    "# Select the numerical feature for One-Class SVM (e.g., 'Value')\n",
    "X = final_df[['Value']]\n",
    "\n",
    "# Standardize the feature\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Fit the One-Class SVM model\n",
    "ocsvm = OneClassSVM(kernel='rbf', gamma='auto', nu=0.01)  # You can adjust the parameters based on your needs\n",
    "ocsvm.fit(X_scaled)\n",
    "\n",
    "# Predict anomalies (1 for normal, -1 for anomaly)\n",
    "final_df['anomaly'] = ocsvm.predict(X_scaled)\n",
    "\n",
    "# Display the first few rows with the anomaly column\n",
    "#final_df[['Value', 'anomaly']].head()\n",
    "\n",
    "# Convert the 'Time' column to datetime format if not already\n",
    "# final_df['Time'] = pd.to_datetime(final_df['Time'])\n",
    "\n",
    "# Plot the values over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(final_df.index, final_df['Value'], label='Value', color='blue')\n",
    "\n",
    "# Highlight anomalies\n",
    "anomalies = final_df[final_df['anomaly'] == -1]\n",
    "plt.scatter(anomalies.index, anomalies['Value'], color='red', label='Anomalies')\n",
    "\n",
    "print(f\"Number of anomalies detected: {len(anomalies)}\")\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.title('duration_milliseconds_bucket One-Class SVM Model')\n",
    "plt.legend()\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOF Model\n",
    "\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "#df = pd.read_csv('your_dataset.csv')  # Use the correct path to your CSV\n",
    "\n",
    "# Extract the 'Value' column for LOF analysis\n",
    "X = final_df[['Value']].values\n",
    "\n",
    "# Apply Local Outlier Factor\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1)  # Adjust parameters as needed\n",
    "final_df['LOF_Score'] = lof.fit_predict(X)\n",
    "\n",
    "# Add a column to indicate if a point is an outlier (-1) or not (1)\n",
    "final_df['Outlier'] = final_df['LOF_Score'] == -1\n",
    "\n",
    "# Display the dataframe with LOF scores and outlier labels\n",
    "print(final_df[['Time', 'Value', 'LOF_Score', 'Outlier']].head())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(final_df.index, final_df['Value'], label='Value', color='blue',  alpha=0.6)\n",
    "\n",
    "# Highlight anomalies\n",
    "anomalies = final_df[final_df['Outlier'] == True]\n",
    "plt.scatter(anomalies.index, anomalies['Value'], color='red', label='Anomalies')\n",
    "\n",
    "print(f\"Number of anomalies detected: {len(anomalies)}\")\n",
    "\n",
    "# Adding labels and title\n",
    "plt.title('duration_milliseconds_bucket (LOF) Model')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA Model\n",
    "\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Load the CSV file\n",
    "# file_path = 'C:/Users/abhis/Desktop/CloudBuilders/Grafana-Anomaly/Grafana-Anomaly_Detection/Anomaly_grafana/dataframes/calls_total_df.csv'\n",
    "# df = pd.read_csv(file_path)\n",
    "\n",
    "# Step 2: Parse the 'Time' column as datetime and set it as the index\n",
    "final_df['Time'] = pd.to_datetime(final_df['Time'])\n",
    "final_df.set_index('Time', inplace=True)\n",
    "\n",
    "# Step 3: Resample the data by summing the 'Value' column every 15 minutes to handle any irregular intervals\n",
    "df_resampled = final_df['Value'].resample('5T').sum()\n",
    "\n",
    "# Step 4: Split the data into training and testing sets (80% train, 20% test)\n",
    "train_size = int(len(df_resampled) * 0.8)\n",
    "train, test = df_resampled[:train_size], df_resampled[train_size:]\n",
    "\n",
    "# Step 5: Fit the ARIMA model on the training data\n",
    "# We'll use a basic ARIMA model with (p, d, q) = (1, 1, 1) as a starting point.\n",
    "model = ARIMA(train, order=(0, 1, 0))\n",
    "arima_result = model.fit()\n",
    "\n",
    "# Step 6: Forecast on the test data\n",
    "forecast = arima_result.forecast(steps=len(test))\n",
    "\n",
    "# Step 7: Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train.index, train, label='Training Data')\n",
    "plt.plot(test.index, test, label='Test Data', color='orange')\n",
    "plt.plot(test.index, forecast, label='ARIMA Forecast', color='green')\n",
    "plt.legend()\n",
    "plt.title('duration_milliseconds_bucket ARIMA Forecast')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prophet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seasonal Decomposition of Time Series (STL)\n",
    "\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Load the dataset\n",
    "# file_path = 'C:/Users/abhis/Desktop/CloudBuilders/Grafana-Anomaly/Grafana-Anomaly_Detection/Anomaly_grafana/dataframes/calls_total_df.csv'  # Update this path as needed\n",
    "# df = pd.read_csv(file_path)\n",
    "df = final_df\n",
    "\n",
    "# Convert 'Time' column to datetime and set it as the index\n",
    "df['Time'] = pd.to_datetime(df['Time'])\n",
    "df.set_index('Time', inplace=True)\n",
    "\n",
    "# Assuming data points are collected every 15 seconds in a 1-hour period\n",
    "period = 240  # 4 data points per minute * 60 minutes = 240 data points per hour\n",
    "\n",
    "# Perform STL decomposition with the specified period\n",
    "stl = STL(df['Value'], period=period)\n",
    "result = stl.fit()\n",
    "\n",
    "# Plot the decomposed components\n",
    "result.plot()\n",
    "plt.show()\n",
    "\n",
    "# Extract residuals from STL decomposition\n",
    "residuals = result.resid\n",
    "\n",
    "# 1. Plot the residuals over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(residuals, label='Residuals')\n",
    "plt.title('Residuals Over Time')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 2. Histogram and KDE plot of residuals\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.histplot(residuals, kde=True, stat=\"density\", linewidth=0)\n",
    "# plt.title('Histogram and KDE of Residuals')\n",
    "# plt.show()\n",
    "\n",
    "# # 3. ACF and PACF plots of residuals\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plot_acf(residuals.dropna(), lags=40)\n",
    "# plt.title('Autocorrelation Function (ACF) of Residuals')\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plot_pacf(residuals.dropna(), lags=40)\n",
    "# plt.title('Partial Autocorrelation Function (PACF) of Residuals')\n",
    "# plt.show()\n",
    "\n",
    "# # 4. Perform Augmented Dickey-Fuller test for stationarity\n",
    "# adf_result = adfuller(residuals.dropna())\n",
    "\n",
    "# # Extracting and displaying ADF test results\n",
    "# adf_output = {\n",
    "#     'ADF Statistic': adf_result[0],\n",
    "#     'p-value': adf_result[1],\n",
    "#     'Number of Lags Used': adf_result[2],\n",
    "#     'Number of Observations Used': adf_result[3]\n",
    "# }\n",
    "\n",
    "# adf_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Autoencoder Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
